# -*- coding: utf-8 -*-
"""new_load_pso.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fEBaxMySFMzMz6RJ49VuTcouwL-OQFBd

# **Import required version of numpy**
"""

!pip install numpy==1.16.1

from numpy.random import seed
seed(1)

"""# **Mount Google Drive**"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd drive/
# %cd My\ Drive
# %cd New_Animal_Detection_Dataset

"""# **Import libraries**"""

# Commented out IPython magic to ensure Python compatibility.
!pip install pyswarms
# Import modules
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris


# Import PySwarms
import pyswarms as ps

# Some more magic so that the notebook will reload external python modules;
# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython
# %load_ext autoreload
# %autoreload 2

import cv2
import numpy as np
from tqdm import tqdm
import os
from random import shuffle
from time import time
# tensorboard --logdir=logs/ --host localhost --port 8088
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
import pandas as pd
import pickle

"""# **Define Train, Validation and Test Directories**"""

TRAIN_DIR = 'Train'
VALIDATION_DIR = 'Validation'
TEST_DIR = 'Test'
EXTRA_DIR = 'Extra_Test'
IMG_SIZE = 50
BATCH_SIZE = 32

"""# **Count Files**"""

SPECIES = ['Deer', 'Fox', 'Raccoon']

for species in SPECIES:
    print('{} {} images'.format(species, len(os.listdir(os.path.join(TRAIN_DIR, species)))))

"""# **View Training Data**"""

trainn = []

for species_num, species in enumerate(SPECIES):
    for file in os.listdir(os.path.join(TRAIN_DIR, species)):
        trainn.append(['Train/{}/{}'.format(species, file), species_num, species])

trainn = pd.DataFrame(trainn, columns=['file', 'species_num', 'species'])

print('Training Data: ',trainn.shape)
print(trainn)

"""# **View Validation Data**"""

validation = []

for species_num, species in enumerate(SPECIES):
    for file in os.listdir(os.path.join(VALIDATION_DIR, species)):
        validation.append(['Validation/{}/{}'.format(species, file), species_num, species])

validation = pd.DataFrame(validation, columns=['file', 'species_num', 'species'])

print('Validation Data: ',validation.shape)
print(validation)

"""# **Utility Functions to fetch Training, Validation and Test Data**"""

def get_training_data():
    """Returns the training data from TRAIN_DIR.
    Images are read in grayscale format and resized to IMG_SIZE dimension square.
    The whole data is saved with numpy in .npy format for quick loading for future purpose.
    """
    training_data = []
    if os.path.isfile('training_data_{}.npy'.format(IMG_SIZE)):
        return np.load('training_data_{}.npy'.format(IMG_SIZE))
    else:
        for species in SPECIES:
          for img in tqdm(os.listdir(os.path.join(TRAIN_DIR, species))):
              label = species
              path = os.path.join(os.path.join(TRAIN_DIR, species) ,img)
              print(path)
              img = cv2.resize(cv2.imread(path,cv2.IMREAD_GRAYSCALE), (IMG_SIZE,IMG_SIZE))
              img = img/255
              training_data.append([np.array(img),np.array(label)])
        shuffle(training_data)
        np.save('training_data_{}.npy'.format(IMG_SIZE),training_data)
        return np.array(training_data)

def get_validation_data():
    """Returns the testing data from VALIDATION_DIR.
    Images are read in grayscale format and resized to IMG_SIZE dimension square.
    The whole data is saved with numpy in .npy format for quick loading for future purpose.
    """
    validation_data = []
    if os.path.isfile('validation_data_{}.npy'.format(IMG_SIZE)):
        return np.load('validation_data_{}.npy'.format(IMG_SIZE))
    else:
        for species in SPECIES:
          for img in tqdm(os.listdir(os.path.join(VALIDATION_DIR, species))):
              label = species
              path = os.path.join(os.path.join(VALIDATION_DIR, species) ,img)
              print(path)
              img = cv2.resize(cv2.imread(path,cv2.IMREAD_GRAYSCALE), (IMG_SIZE,IMG_SIZE))
              img = img/255
              validation_data.append([np.array(img),np.array(label)])
        shuffle(validation_data)
        np.save('validation_data_{}.npy'.format(IMG_SIZE),validation_data)
        return np.array(validation_data)

def get_test_data():
    """Returns the testing data from TEST_DIR.
    Images are read in grayscale format and resized to IMG_SIZE dimension square.
    The whole data is saved with numpy in .npy format for quick loading for future purpose.
    """
    test_data = []
    if os.path.isfile('test_data_{}.npy'.format(IMG_SIZE)):
        return np.load('test_data_{}.npy'.format(IMG_SIZE))
    else:
        for img in tqdm(os.listdir(TEST_DIR)):
            #img_id = int(img.split('.')[0])
            path = os.path.join(TEST_DIR,img)
            img = cv2.resize(cv2.imread(path,cv2.IMREAD_GRAYSCALE), (IMG_SIZE,IMG_SIZE))
            img = img/255
            test_data.append([np.array(img),"anything"])
        test_data.sort(key = lambda x: x[1])
        np.save('test_data_{}.npy'.format(IMG_SIZE),test_data)
        return np.array(test_data)

"""# **Training Set**"""

data = get_training_data()
print(data.shape)

train = data[:3597]    # For Training purpose

# Training set
X_train = np.array([i[0] for i in train])

for i in train:
  string = i[1]
  if string=='Deer':
    i[1] = 0
  if string =='Fox':
    i[1] = 1
  if string == 'Raccoon':
    i[1] = 2

y_train = np.array([i[1] for i in train])
X_train = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE*IMG_SIZE)/255.

print(X_train.shape)
print(y_train.shape)

X = X_train
y = y_train
print(X.shape)
print(y.shape)

"""# **Forward propagation as objective function**"""

# Forward propagation
def forward_prop(params):
    """Forward propagation as objective function

    This computes for the forward propagation of the neural network, as
    well as the loss. It receives a set of parameters that must be
    rolled-back into the corresponding weights and biases.

    Inputs
    ------
    params: np.ndarray
        The dimensions should include an unrolled version of the
        weights and biases.

    Returns
    -------
    float
        The computed negative log-likelihood loss given the parameters
    """
    # Neural network architecture
    n_inputs = 2500
    n_hidden = 20
    n_classes = 3

    # Roll-back the weights and biases
    W1 = params[0:50000].reshape((n_inputs,n_hidden))
    b1 = params[50000:50020].reshape((n_hidden,))
    W2 = params[50020:50080].reshape((n_hidden,n_classes))
    b2 = params[50080:50083].reshape((n_classes,))

    # Perform forward propagation
    z1 = X.dot(W1) + b1  # Pre-activation in Layer 1
    a1 = np.tanh(z1)     # Activation in Layer 1
    z2 = a1.dot(W2) + b2 # Pre-activation in Layer 2
    logits = z2          # Logits for Layer 2

    # Compute for the softmax of the logits
    exp_scores = np.exp(logits)
    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

    # Compute for the negative log likelihood
    N = 1500 # Number of samples
    corect_logprobs = -np.log(probs[range(N), y])
    loss = np.sum(corect_logprobs) / N

    return loss

"""# **Higher-level method to do forward_prop in the whole swarm**"""

def f(x):
    """Higher-level method to do forward_prop in the
    whole swarm.

    Inputs
    ------
    x: numpy.ndarray of shape (n_particles, dimensions)
        The swarm that will perform the search

    Returns
    -------
    numpy.ndarray of shape (n_particles, )
        The computed loss for each particle
    """
    n_particles = x.shape[0]
    j = [forward_prop(x[i]) for i in range(n_particles)]
    return np.array(j)

"""# **Initialize swarm, Call instance of PSO**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Initialize swarm
# options = {'c1': 0.5, 'c2': 0.3, 'w':0.9}
# 
# # Call instance of PSO
# dimensions = (2500 * 20) + (20 * 3) + 20 + 3
# optimizer = ps.single.GlobalBestPSO(n_particles=40, dimensions=dimensions, options=options)
# 
# '''
# # Perform optimization
# cost, pos = optimizer.optimize(f, iters=1000)
# '''
#

'''
a = [cost, pos]
# write object to file
pickle.dump(a, open('pso_weights.txt', 'wb'))
'''

"""# **Load saved parameters**"""

import pickle
# read object from file
b = pickle.load(open('experiment_pso_weights.txt', 'rb'))
loaded_cost = b[0]
loaded_pos = b[1]
print(loaded_cost)
print(loaded_pos)
cost = loaded_cost
pos = loaded_pos

"""# **Perform class predictions**"""

def predict(X, pos):
    """
    Use the trained weights to perform class predictions.

    Inputs
    ------
    X: numpy.ndarray

    pos: numpy.ndarray
        Position matrix found by the swarm. Will be rolled
        into weights and biases.
    """
    # Neural network architecture
    n_inputs = 2500
    n_hidden = 20
    n_classes = 3

    # Roll-back the weights and biases

    W1 = pos[0:50000].reshape((n_inputs,n_hidden))
    b1 = pos[50000:50020].reshape((n_hidden,))
    W2 = pos[50020:50080].reshape((n_hidden,n_classes))
    b2 = pos[50080:50083].reshape((n_classes,))

    # Perform forward propagation
    z1 = X.dot(W1) + b1  # Pre-activation in Layer 1
    a1 = np.tanh(z1)     # Activation in Layer 1
    z2 = a1.dot(W2) + b2 # Pre-activation in Layer 2
    logits = z2          # Logits for Layer 2

    y_pred = np.argmax(logits, axis=1)
    return y_pred

"""# **Train Accuracy**"""

accuracy = accuracy_score(y, predict(X, pos))
#print(y_val)
print("Train Accuracy:%.2f%%"%(accuracy * 100.0))

"""# **Validation Set**"""

# Predictions on extra Data# Validation set
validation_data = get_validation_data()
print(validation_data.shape)

partition = 633            # Breaking -ve index
validation = data[:partition]    # For Training purpose

X_val = np.array([i[0] for i in validation])
X_val = np.array([i[0] for i in validation]).reshape(-1,IMG_SIZE*IMG_SIZE)/255. #.reshape((1, -1))

for i in validation:
  string = i[1]
  if string=='coati':
    i[1] = 0
  if string =='roe_deer':
    i[1] = 1
  if string == 'wild_boar':
    i[1] = 2

y_val = np.array([i[1] for i in validation])
print(X_val.shape)
print(y_val.shape)

X = X_val
y = y_val

"""# **Validation Accuracy**"""

validation_accuracy = accuracy_score(y, predict(X, pos))
print("Validation Accuracy:%.2f%%"%(validation_accuracy * 100.0))

import pickle
# read object from file
loaded_validation_acc = pickle.load(open('experiment_pso_accuracy.txt', 'rb'))
print("Loaded Validation Accuracy = ", loaded_validation_acc*100, "%")
loaded_training_acc = pickle.load(open('train_pso_accuracy.txt', 'rb'))
print("Loaded Training Accuracy = ", loaded_training_acc*100, "%")
'''
if loaded_validation_acc < validation_accuracy:
  a = [cost, pos]
  # write object to file
  pickle.dump(a, open('experiment_pso_weights.txt', 'wb'))
  pickle.dump(validation_accuracy, open('experiment_pso_accuracy.txt', 'wb'))
'''

"""
# **Prediction on Test Data**"""

test_data = get_test_data()
print(test_data.shape)
X_test = np.array([i[0] for i in test_data]).reshape(-1,IMG_SIZE*IMG_SIZE)/255. #.reshape((1, -1))
pred = predict(X_test, pos)
print(pred)

pred_list = []
for i in pred:
  if i==0:
    prediction = 'Deer'
  elif i == 1:
    prediction = 'Fox'
  elif i== 2:
    prediction = 'Raccoon'
  else:
    prediction = 'none'
  pred_list.append(prediction)

print(pred_list)

"""# **Performance Report**"""

def get_img_data():
    files = []
    for img in tqdm(os.listdir(TEST_DIR)):
      #img_id = int(img.split('.')[0])
      files.append(img.split('.')[0])
    return files

files = get_img_data()
print(files)
for i in range(0, len(files)):
  if 'Deer' in files[i]:
    files[i] = 'Deer'
  if 'Fox' in files[i]:
    files[i] = 'Fox'
  if 'Raccoon' in files[i]:
    files[i] = 'Raccoon'
print(files)
y_test = files
print(y_test)

# Python script for confusion matrix creation.
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

actual = y_test
predicted =  pred_list
results = confusion_matrix(actual, predicted)

print('Confusion Matrix :')
print(results)
print('Accuracy Score :',accuracy_score(actual, predicted))
print('Report : ')
print(classification_report(actual, predicted))

"""# **Bounding Boxes**"""

import cv2
bb_model = 'MobileNetSSD_deploy.caffemodel'
prototxt = 'MobileNetSSD_deploy.prototxt.txt'
# image = 'images/example_05.jpg'
c_confidence = .5

# initialize the list of class labels MobileNet SSD was trained to
# detect, then generate a set of bounding box colors for each class

CLASSES = ["background", "aeroplane", "bicycle", "bird", "boat",
	"bottle", "bus", "car", "cat", "chair", "cow", "diningtable",
	"dog", "horse", "motorbike", "person", "pottedplant", "sheep",
	"sofa", "train", "tvmonitor"]

COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))

# load our serialized model from disk
# print("[INFO] loading model...")
net = cv2.dnn.readNetFromCaffe(prototxt, bb_model)

def get_extra_data():
    """Returns the testing data from TEST_DIR.
    Images are read in grayscale format and resized to IMG_SIZE dimension square.
    The whole data is saved with numpy in .npy format for quick loading for future purpose.
    """
    extra_data = []
    if os.path.isfile('extra_data_{}.npy'.format(IMG_SIZE)):
        return np.load('extra_data_{}.npy'.format(IMG_SIZE))
    else:
        for img in tqdm(os.listdir(EXTRA_DIR)):
            #img_id = int(img.split('.')[0])
            path = os.path.join(EXTRA_DIR,img)
            img = cv2.resize(cv2.imread(path,cv2.IMREAD_GRAYSCALE), (IMG_SIZE,IMG_SIZE))
            img = img/255
            extra_data.append([np.array(img),"anything"])
        extra_data.sort(key = lambda x: x[1])
        np.save('extra_data_{}.npy'.format(IMG_SIZE),extra_data)
        return np.array(extra_data)

#Prediction on Extra Data
extra_data = get_extra_data()
X_extra = np.array([i[0] for i in extra_data]).reshape(-1,IMG_SIZE*IMG_SIZE)/255.
#ids = [i[1] for i in single_data]
pred = predict(X_extra, pos)
print(pred)
